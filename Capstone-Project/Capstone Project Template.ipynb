{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "I94 Immigration data and city temperature data will be used to create a database that is optimized to query and analyze immigration events. An ETL pipeline is to be build with these to data sources to create the database. Finally, the database will be used to access immigration behaviour to location temperatures.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sql_queries import airport_insert, demographic_insert, immigration_insert, temperature_insert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "This projects aims to enrich the US I94 immigration data with further data such as demographics and temperature data to have a wider basis for analysis on the immigration data.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. This is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "i94_path = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_i94 = pd.read_sas(i94_path, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_i94.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## World Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temp = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# find all unique country codes in temperature data to find used name for United States \n",
    "set(df_temp[\"Country\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp_us = df_temp[df_temp[\"Country\"] == \"United States\"]\n",
    "df_temp_us.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics = pd.read_csv(\"us-cities-demographics.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Airport Code Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes = pd.read_csv(\"airport-codes_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "df_spark.write.parquet(\"sas_data\")\n",
    "df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_i94.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp_us.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Cleaning Steps\n",
    "Document steps necessary to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get port locations from SAS text file\n",
    "with open(\"./I94_SAS_Labels_Descriptions.SAS\") as f:\n",
    "    content = f.readlines()\n",
    "content = [x.strip() for x in content]\n",
    "ports = content[302:962]\n",
    "splitted_ports = [port.split(\"=\") for port in ports]\n",
    "port_codes = [x[0].replace(\"'\",\"\").strip() for x in splitted_ports]\n",
    "port_locations = [x[1].replace(\"'\",\"\").strip() for x in splitted_ports]\n",
    "port_cities = [x.split(\",\")[0] for x in port_locations]\n",
    "port_states = [x.split(\",\")[-1] for x in port_locations]\n",
    "df_port_locations = pd.DataFrame({\"port_code\" : port_codes, \"port_city\": port_cities, \"port_state\": port_states})\n",
    "df_port_locations.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print first and last element in dict to check if all lines in file are covered\n",
    "print(f\"First port in SAS file: {df_port_locations['port_city'].values[0]}, last port {df_port_locations['port_city'].values[-1]}\")\n",
    "irregular_ports_df = df_port_locations[df_port_locations[\"port_city\"] == df_port_locations[\"port_state\"]]\n",
    "irregular_ports = list(set(irregular_ports_df[\"port_code\"].values))\n",
    "print(irregular_ports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop all irregular ports from i94 data\n",
    "print(f\"i94 data contains {len(df_i94)} rows before cleaning.\")\n",
    "df_i94_filtered = df_i94[~df_i94[\"i94port\"].isin(irregular_ports)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(f\"i94 data contains {len(df_i94_filtered)} rows after removing irregular ports.\")\n",
    "df_i94_filtered.drop(columns=[\"insnum\", \"entdepu\", \"occup\", \"visapost\"], inplace=True)\n",
    "df_i94_filtered.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(f\"i94 data contains {len(df_i94_filtered)} rows after removing NaN values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clear missing temperature values\n",
    "df_temp_us.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes.dropna(subset=['iata_code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "##### Tables:\n",
    "| table name | columns | description | type |\n",
    "| ------- | ---------- | ----------- | ---- |\n",
    "| airports | iata_code - name - type - local_code - coordinates - city | stores information related to airports | dimension table |\n",
    "| demographics | city - state - media_age - male_population - female_population - total_population - num_veterans - foreign_born - average_household_size - state_code - race - count | stores demographics data for cities | dimension table |\n",
    "| immigrations | cicid - year - month - cit - res - iata - arrdate - mode - addr - depdate - bir - visa - coun- dtadfil - visapost - occup - entdepa - entdepd - entdepu - matflag - biryear - dtaddto - gender - insnum - airline - admnum - fltno - visatype | stores all i94 immigrations data | fact table |\n",
    "| temperature | timestamp - average_temperature - average_temperatur_uncertainty - city - country - latitude - longitude | stores temperature information | dimension table |\n",
    "\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "1. Create tables by executing `create_tables.py`.\n",
    "2. Join city to airports data.\n",
    "3. Insert data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# After running create_tables.py, insert the data into the database\n",
    "conn = psycopg2.connect(\"host=127.0.0.1 dbname=sparkifydb user=student password=student\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes = df_airport_codes.merge(df_port_locations, left_on=\"iata_code\", right_on=\"port_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes.drop(columns=[\"port_code\"], inplace=True)\n",
    "df_airport_codes = df_airport_codes[[\"iata_code\", \"name\", \"type\", \"local_code\", \"coordinates\", \"port_city\", \"elevation_ft\", \"continent\", \"iso_country\", \"iso_region\", \"municipality\", \"gps_code\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_airport_codes.iterrows():\n",
    "    cur.execute(airport_insert, list(row.values))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_demographics.iterrows():\n",
    "    cur.execute(demographic_insert, list(row.values))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for index, row in df_i94_filtered.iterrows():\n",
    "    cur.execute(immigration_insert, list(row.values))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "for index, row in df_temp_us.iterrows():\n",
    "    cur.execute(temperature_insert, list(row.values))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "cur.execute(\"SELECT COUNT(*) FROM airports\")\n",
    "conn.commit()\n",
    "if cur.rowcount < 1:\n",
    "    print(\"No data found in table airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT COUNT(*) FROM demographics\")\n",
    "conn.commit()\n",
    "if cur.rowcount < 1:\n",
    "    print(\"No data found in table demographics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT COUNT(*) FROM immigrations\")\n",
    "conn.commit()\n",
    "if cur.rowcount < 1:\n",
    "    print(\"No data found in table immigrations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT COUNT(*) FROM temperature\")\n",
    "conn.commit()\n",
    "if cur.rowcount < 1:\n",
    "    print(\"No data found in table temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "**Fact Table** - I94 immigration data joined with the city temperature data on i94port\n",
    "Columns:\n",
    "   - i94yr = 4 digit year,\n",
    "   - i94mon = numeric month,\n",
    "   - i94cit = 3 digit code of origin city,\n",
    "   - i94port = 3 character code of destination USA city,\n",
    "   - arrdate = arrival date in the USA,\n",
    "   - i94mode = 1 digit travel code,\n",
    "   - depdate = departure date from the USA,\n",
    "   - i94visa = reason for immigration,\n",
    "   - AverageTemperature = average temperature of destination city,\n",
    "\n",
    "**Dimension Table** - I94 immigration data Events\n",
    "Columns:\n",
    "   - i94yr = 4 digit year\n",
    "   - i94mon = numeric month\n",
    "   - i94cit = 3 digit code of origin city\n",
    "   - i94port = 3 character code of destination USA city\n",
    "   - arrdate = arrival date in the USA\n",
    "   - i94mode = 1 digit travel code\n",
    "   - depdate = departure date from the USA\n",
    "   - i94visa = reason for immigration\n",
    "\n",
    "**Dimension Table** - temperature data\n",
    "Columns:\n",
    "- i94port = 3 character code of destination city (mapped from cleaned up immigration data)\n",
    "- AverageTemperature = average temperature\n",
    "- City = city name\n",
    "- Country = country name\n",
    "- Latitude= latitude\n",
    "- Longitude = longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project:\n",
    "  1. I used Spark since it can easily handle multiple file formats (SAS, csv, etc) that contain large amounts of data. Spark SQL was used to process the input files into dataframes and manipulated via standard SQL join operations to create the tables.\n",
    "    \n",
    "* Propose how often the data should be updated and why.\n",
    "    1. Since the format of the raw files are monthly, we should continue pulling the data monthly.\n",
    "    \n",
    "### Scenarios\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    "    1. the data was increased by 100x.\n",
    "        - Use Amazon Redshift: It is an analytical database that is optimized for aggregation and read-heavy workloads\n",
    "    2. The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "        - Airflow can be used here, create DAG retries or send emails on failures.\n",
    "        - Have daily quality checks; if fail, send emails to operators and freeze dashboards\n",
    "    3. The database needed to be accessed by 100+ people.\n",
    "        - Redshift can help us here since it has auto-scaling capabilities and good read performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
